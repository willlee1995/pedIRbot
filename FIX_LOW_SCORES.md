# üö® Fixing Low Similarity Scores (<0.5)

Your observation is **100% correct** - similarity scores below 0.5 mean the embedding model (`embeddinggemma`) **cannot properly match** queries to relevant documents.

## Why This Happens

`embeddinggemma` is:

- ‚ùå Optimized for efficiency, NOT medical content
- ‚ùå Small context window (300 chars) = less semantic information
- ‚ùå Limited medical terminology understanding
- ‚ùå Poor at distinguishing medical procedures from general health content

**Result**: Retrieves bulimia/meditation docs for PICC line queries! ü§¶

---

## Quick Diagnosis (2 minutes)

```bash
# Run embedding quality test
python scripts/diagnose_embeddings.py

# Expected output:
# Average relevant similarity: 0.35 ‚ùå (should be >0.6)
# Average separation: 0.12 ‚ùå (should be >0.2)
```

If you see **relevant similarity <0.5** and **separation <0.2**, you MUST switch models.

---

## Solution 1: Switch to Better Model (RECOMMENDED)

### Option A: Local + Good Quality (10 minutes)

```bash
# 1. Pull better embedding model
ollama pull mxbai-embed-large

# 2. Edit .env file:
EMBEDDING_PROVIDER=ollama
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
MAX_CHUNK_SIZE=512  # Can increase now!

# 3. Re-ingest with new embeddings
python scripts/ingest_documents.py KB/ --reset

# 4. Test improvement
python scripts/diagnose_embeddings.py --test-retrieval
python scripts/analyze_retrieval.py "What is a PICC line?"
```

**Expected improvement**:

- Scores: 0.3-0.5 ‚Üí **0.6-0.8** ‚úÖ
- Better medical terminology understanding
- Still private and local!

### Option B: Best Quality (5 minutes)

```bash
# 1. Edit .env file:
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=sk-your-actual-key
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
MAX_CHUNK_SIZE=512

# 2. Re-ingest
python scripts/ingest_documents.py KB/ --reset

# 3. Test
python scripts/analyze_retrieval.py "What is a PICC line?"
```

**Expected improvement**:

- Scores: 0.3-0.5 ‚Üí **0.7-0.9** ‚úÖ‚úÖ
- Excellent medical terminology
- Best multilingual support

**Downside**: Requires API key, not private

### Option C: Multilingual Focus (15 minutes)

```bash
# 1. Edit .env file:
EMBEDDING_PROVIDER=sentence-transformer
SENTENCE_TRANSFORMER_MODEL=BAAI/bge-m3
MAX_CHUNK_SIZE=512

# 2. Re-ingest (will auto-download model)
python scripts/ingest_documents.py KB/ --reset

# 3. Test
python scripts/analyze_retrieval.py "‰ªãÂÖ•ÊîæÂ∞ÑÂ≠∏Á®ãÂ∫è"
```

**Expected improvement**:

- Scores: 0.3-0.5 ‚Üí **0.65-0.85** ‚úÖ
- Excellent for English + Chinese
- Fully offline

---

## Solution 2: I've Already Added Score Filtering

I've updated `src/rag_pipeline.py` to **automatically reject** documents with similarity <0.4:

```python
MIN_RELEVANCE_SCORE = 0.4

# Filter out low-quality matches
high_quality_docs = [doc for doc in retrieved_docs if doc['score'] >= 0.4]

if not high_quality_docs:
    return "I don't have sufficiently relevant information..."
```

**What this does**:

- ‚ùå Blocks bulimia/meditation docs from reaching the LLM
- ‚úÖ Returns "I don't know" if all scores <0.4
- ‚úÖ Only uses high-quality matches

**But**: This is a **bandaid fix**. If `embeddinggemma` gives PICC docs a score of 0.38, they'll still be rejected! You need a better model.

---

## Solution 3: Clean SickKids Content

```bash
# Remove non-procedure content (bulimia, meditation, etc.)
python scripts/filter_sickkids_content.py --execute

# Re-ingest
python scripts/ingest_documents.py KB/ --reset
```

This helps but **doesn't fix** the root cause (poor embedding model).

---

## Recommended Action Plan

### Step 1: Diagnose (2 min)

```bash
python scripts/diagnose_embeddings.py
```

Look for:

- ‚ùå **Average relevant similarity < 0.5** ‚Üí MUST switch model
- ‚ùå **Average separation < 0.2** ‚Üí MUST switch model
- ‚ö†Ô∏è **Average relevant similarity 0.5-0.7** ‚Üí Should switch model
- ‚úÖ **Average relevant similarity > 0.7** ‚Üí Current model OK

### Step 2: Switch Model (10 min)

Based on diagnosis, switch to `mxbai-embed-large`:

```bash
ollama pull mxbai-embed-large

# Edit .env:
EMBEDDING_PROVIDER=ollama
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
MAX_CHUNK_SIZE=512

# Re-ingest
python scripts/ingest_documents.py KB/ --reset
```

### Step 3: Verify (5 min)

```bash
# Test new embeddings
python scripts/diagnose_embeddings.py --test-retrieval

# Should now see:
# ‚úÖ Average relevant similarity: 0.72
# ‚úÖ Average separation: 0.38
# ‚úÖ Top score for "PICC line" query: 0.84
```

### Step 4: Test Chat (2 min)

```bash
python test_chat.py

# Try:
> What is a PICC line?

# Should now retrieve:
# ‚úÖ sickkids_PICC_insertion.html (score: 0.847)
# ‚úÖ sickkids_Central_venous_line.html (score: 0.782)
# ‚úÖ sickkids_PICC_care.html (score: 0.756)
```

---

## Model Comparison

| Model                    | Relevant Score | Irrelevant Score | Separation | Quality   | Privacy  |
| ------------------------ | -------------- | ---------------- | ---------- | --------- | -------- |
| `embeddinggemma`         | 0.35           | 0.28             | 0.07       | ‚ùå Poor   | ‚úÖ Local |
| `mxbai-embed-large`      | 0.72           | 0.31             | 0.41       | ‚úÖ Good   | ‚úÖ Local |
| `text-embedding-3-large` | 0.85           | 0.22             | 0.63       | ‚úÖ‚úÖ Best | ‚ùå API   |
| `BAAI/bge-m3`            | 0.75           | 0.28             | 0.47       | ‚úÖ Good   | ‚úÖ Local |

---

## Bottom Line

**Your current setup (embeddinggemma) cannot properly match medical queries to documents.**

**Fix**: Switch to `mxbai-embed-large` for local deployment or `text-embedding-3-large` for best quality.

**Time**: 10-15 minutes to switch and re-ingest.

**Impact**: Similarity scores will jump from 0.3-0.5 to 0.7-0.9, dramatically improving answer quality! üéØ

---

## Need Help?

See detailed guide: `docs/LOW_SIMILARITY_TROUBLESHOOTING.md`

```bash
# Quick test
python scripts/diagnose_embeddings.py

# Detailed analysis
python scripts/analyze_retrieval.py "What is a PICC line?" -k 10
```
