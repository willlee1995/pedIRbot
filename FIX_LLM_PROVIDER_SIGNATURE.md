# ðŸ”´ CRITICAL FIX: Wrong LLM Provider Method Signature

## The REAL Problem

The script was calling the LLM provider with the **wrong parameter type**.

### What We Were Doing (âŒ WRONG)
```python
response = self.llm_provider.generate(
    prompt=prompt,  # âŒ Passing a STRING directly
    temperature=0.3,
    max_tokens=500
)
```

### What It Actually Expects (âœ… CORRECT)
```python
response = self.llm_provider.generate(
    messages=[  # âœ… Expects a LIST of MESSAGE DICTS
        {"role": "system", "content": "..."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.3,
    max_tokens=500
)
```

---

## Why This Caused TypeError

Looking at `src/llm.py` line 17:

```python
def generate(self, messages: List[Dict[str, str]], **kwargs) -> str:
    """
    Args:
        messages: List of message dicts with 'role' and 'content'  â† Expects THIS
        **kwargs: Additional parameters
    """
```

**Calling signature mismatch:**
```
Expected:  generate(messages=List[Dict], **kwargs)
We called: generate(prompt=str, **kwargs)
                    â†‘ Parameter name is wrong!
```

This caused:
1. Missing required `messages` parameter â†’ TypeError
2. TypeError wrapped in RetryError by tenacity
3. Script fails

---

## The Fix: Use Correct Message Format

### Before (âŒ WRONG)
```python
response = self.llm_provider.generate(
    prompt=prompt,  # âŒ Wrong parameter
    temperature=0.3,
    max_tokens=500
)
```

### After (âœ… CORRECT)
```python
messages = [
    {"role": "system", "content": "You are a pediatric IR expert..."},
    {"role": "user", "content": prompt}
]

response = self.llm_provider.generate(
    messages=messages,  # âœ… Correct parameter
    temperature=0.3,
    max_tokens=500
)
```

---

## Message Format Structure

The LLM provider expects a conversation-like format:

```python
messages = [
    {
        "role": "system",      # System instructions
        "content": "You are a pediatric IR expert helping parents..."
    },
    {
        "role": "user",        # User's actual question/prompt
        "content": "Q: Why is treatment recommended?\n\nContext: ..."
    }
]
```

This is the **standard OpenAI/LLM API format**, which all providers expect.

---

## Error Flow Explained

```
1. Script calls: generate(prompt="...", temperature=0.3, max_tokens=500)
                          â†“
2. LLM provider expects: generate(messages=[...], **kwargs)
                          â†“
3. Mismatch! TypeError: "missing required keyword argument 'messages'"
                          â†“
4. Tenacity retry wrapper catches it â†’ wraps in RetryError
                          â†“
5. Retries 3 times (all fail with same error)
                          â†“
6. Final exception logged as: RetryError[... raised TypeError>]
```

---

## Why Previous Fixes Didn't Work

The previous retry/fallback fixes were treating symptoms, not the root cause:

```python
# âŒ This tried to catch the exception, but...
try:
    response = self.llm_provider.generate(prompt=prompt, ...)  # â† Still wrong!
except Exception:
    # Fallback
```

**The problem was still there** - we were still calling with wrong parameters!

---

## Current Implementation (FIXED)

Now properly structured:

```python
# Build correct message format
messages = [
    {"role": "system", "content": "Expert prompt..."},
    {"role": "user", "content": prompt}
]

try:
    # Call with correct parameter name
    response = self.llm_provider.generate(
        messages=messages,  # âœ… CORRECT!
        temperature=0.3,
        max_tokens=500
    )
except TypeError:
    # If LLM provider fails, fall back to direct Ollama
    response = self._call_ollama_direct(prompt)
```

---

## Expected Behavior Now

### Before (Failed)
```
WARNING | LLM generation failed (attempt 1): RetryError[...TypeError>]
WARNING | LLM generation failed (attempt 2): RetryError[...TypeError>]
WARNING | LLM generation failed (attempt 3): RetryError[...TypeError>]
```

### After (Works!)
```
DEBUG | Using LLM provider: OpenAIProvider
DEBUG | âœ“ Generated answer for Angioplasty And Stent Eng
âœ… Generated 10 Q&A pairs
```

---

## Testing

```bash
python scripts/curate_with_medgemma.py

# Should see:
# âœ“ Generated answer for [procedure]
# âœ… Generated 10 Q&A pairs
# (No more RetryError/TypeError warnings!)
```

---

## Code Changes Summary

| File | Line | Change |
|------|------|--------|
| `scripts/curate_with_medgemma.py` | ~305-310 | Changed `prompt=` to `messages=` |
| `scripts/curate_with_medgemma.py` | ~303-308 | Build proper message format |
| `scripts/curate_with_medgemma.py` | ~311-320 | Simplified error handling |

---

## Why This Is The Real Fix

âœ… **Addresses root cause** - Wrong parameter type
âœ… **Matches LLM API contract** - Uses messages format
âœ… **Works with all providers** - Standard format
âœ… **No more TypeError** - Correct signature
âœ… **Clean fallback** - Uses direct Ollama if provider fails

---

## Status

ðŸŽ‰ **FIXED**

The script now calls the LLM provider with the correct method signature!

Try running again and it should work perfectly!

